{"cells":[{"cell_type":"code","execution_count":234,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-12T17:41:47.160477Z","iopub.status.busy":"2024-08-12T17:41:47.160007Z","iopub.status.idle":"2024-08-12T17:41:47.188155Z","shell.execute_reply":"2024-08-12T17:41:47.186976Z","shell.execute_reply.started":"2024-08-12T17:41:47.160441Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/digit-recognizer/sample_submission.csv\n","/kaggle/input/digit-recognizer/train.csv\n","/kaggle/input/digit-recognizer/test.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split \n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":235,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:41:47.190505Z","iopub.status.busy":"2024-08-12T17:41:47.190106Z","iopub.status.idle":"2024-08-12T17:41:52.533041Z","shell.execute_reply":"2024-08-12T17:41:52.531869Z","shell.execute_reply.started":"2024-08-12T17:41:47.190473Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImageId</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>27995</th>\n","      <td>27996</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27996</th>\n","      <td>27997</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27997</th>\n","      <td>27998</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27998</th>\n","      <td>27999</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27999</th>\n","      <td>28000</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28000 rows × 2 columns</p>\n","</div>"],"text/plain":["       ImageId  Label\n","0            1      0\n","1            2      0\n","2            3      0\n","3            4      0\n","4            5      0\n","...        ...    ...\n","27995    27996      0\n","27996    27997      0\n","27997    27998      0\n","27998    27999      0\n","27999    28000      0\n","\n","[28000 rows x 2 columns]"]},"execution_count":235,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n","test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n","submission=pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\n","submission"]},{"cell_type":"code","execution_count":236,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:41:52.534589Z","iopub.status.busy":"2024-08-12T17:41:52.534243Z","iopub.status.idle":"2024-08-12T17:41:53.025513Z","shell.execute_reply":"2024-08-12T17:41:53.024545Z","shell.execute_reply.started":"2024-08-12T17:41:52.534561Z"},"trusted":true},"outputs":[],"source":["y=train[\"label\"].to_numpy()\n","x=train.drop(columns=['label']).to_numpy()\n","x_train, x_test, y_train, y_test = train_test_split(\n","    x, y, test_size=0.20, random_state=0)"]},{"cell_type":"code","execution_count":237,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:41:53.028189Z","iopub.status.busy":"2024-08-12T17:41:53.027841Z","iopub.status.idle":"2024-08-12T17:41:53.036838Z","shell.execute_reply":"2024-08-12T17:41:53.035650Z","shell.execute_reply.started":"2024-08-12T17:41:53.028161Z"},"trusted":true},"outputs":[],"source":["class Linear:\n","    def __init__(self, input_dim, output_dim):\n","        \"\"\"\n","        Initialize the Linear layer with random weights and zero biases.\n","\n","        Parameters:\n","        input_dim (int): Dimension of the input data.\n","        output_dim (int): Dimension of the output data.\n","        \"\"\"\n","        self.W = np.random.randn(input_dim, output_dim) * 0.01\n","        self.b = np.zeros((1, output_dim))\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Perform the forward pass of the Linear layer.\n","\n","        Parameters:\n","        X (numpy.ndarray): Input data.\n","\n","        Returns:\n","        numpy.ndarray: Output of the forward pass.\n","        \"\"\"\n","        self.X = X\n","        return np.dot(X, self.W) + self.b\n","\n","    def backward(self, dA):\n","        \"\"\"\n","        Perform the backward pass of the Linear layer.\n","\n","        Parameters:\n","        dA (numpy.ndarray): Gradient of the loss with respect to the output.\n","\n","        Returns:\n","        numpy.ndarray: Gradient of the loss with respect to the input.\n","        \"\"\"\n","        m = self.X.shape[0]\n","        self.dW = np.dot(self.X.T, dA) / m\n","        self.db = np.sum(dA, axis=0, keepdims=True) / m\n","        return np.dot(dA, self.W.T)"]},{"cell_type":"code","execution_count":239,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:41:53.050326Z","iopub.status.busy":"2024-08-12T17:41:53.050000Z","iopub.status.idle":"2024-08-12T17:41:53.064106Z","shell.execute_reply":"2024-08-12T17:41:53.063053Z","shell.execute_reply.started":"2024-08-12T17:41:53.050300Z"},"trusted":true},"outputs":[],"source":["class ReLu:\n","    def forward(self, inputs):\n","        \"\"\"\n","        Compute the forward pass of the ReLU activation function.\n","\n","        Args:\n","            inputs (numpy.ndarray): The input array for the ReLU activation.\n","\n","        Returns:\n","            numpy.ndarray: The output of the ReLU activation (same shape as input).\n","        \"\"\"\n","        # Store the input for use in backward pass\n","        self.input = inputs\n","        # Compute the ReLU activation: max(0, input)\n","        self.output = np.maximum(0, inputs)\n","        return self.output\n","        \n","    def backward(self, gradient_output):\n","        \"\"\"\n","        Compute the backward pass (gradient) of the ReLU activation function.\n","\n","        Args:\n","            gradient_output (numpy.ndarray): The gradient of the loss with respect to the output of this layer.\n","\n","        Returns:\n","            numpy.ndarray: The gradient of the loss with respect to the input of this layer.\n","        \"\"\"\n","        # Compute the gradient of the ReLU function\n","        # If the input was positive, the gradient is 1; otherwise, it is 0\n","        self.diffv = np.where(self.input > 0, gradient_output, 0)\n","        return self.diffv\n","\n","\n","class Sigmoid:\n","    def forward(self, input):\n","        \"\"\"\n","        Compute the forward pass of the Sigmoid activation function.\n","\n","        Args:\n","            input (numpy.ndarray): The input array for the Sigmoid activation.\n","\n","        Returns:\n","            numpy.ndarray: The output of the Sigmoid activation (same shape as input).\n","        \"\"\"\n","        # Compute the Sigmoid activation: 1 / (1 + exp(-input))\n","        self.output = 1 / (1 + np.exp(-input))\n","        return self.output\n","        \n","    def backward(self, gradient_output):\n","        \"\"\"\n","        Compute the backward pass (gradient) of the Sigmoid activation function.\n","\n","        Args:\n","            gradient_output (numpy.ndarray): The gradient of the loss with respect to the output of this layer.\n","\n","        Returns:\n","            numpy.ndarray: The gradient of the loss with respect to the input of this layer.\n","        \"\"\"\n","        # Compute the gradient of the Sigmoid function\n","        # The gradient is: gradient_output * (1 - output) * output\n","        self.diffv = gradient_output * (1 - self.output) * self.output\n","        return self.diffv\n","\n","\n","class Tanh:\n","    def forward(self, input):\n","        \"\"\"\n","        Compute the forward pass of the Tanh activation function.\n","\n","        Args:\n","            input (numpy.ndarray): The input array for the Tanh activation.\n","\n","        Returns:\n","            numpy.ndarray: The output of the Tanh activation (same shape as input).\n","        \"\"\"\n","        # Store the input for use in backward pass\n","        self.input = input\n","        # Compute the Tanh activation: np.tanh(input)\n","        self.output = np.tanh(self.input)\n","        return self.output\n","        \n","    def backward(self, gradient_output):\n","        \"\"\"\n","        Compute the backward pass (gradient) of the Tanh activation function.\n","\n","        Args:\n","            gradient_output (numpy.ndarray): The gradient of the loss with respect to the output of this layer.\n","\n","        Returns:\n","            numpy.ndarray: The gradient of the loss with respect to the input of this layer.\n","        \"\"\"\n","        # Compute the gradient of the Tanh function\n","        # The gradient is: gradient_output * (1 - np.power(output, 2))\n","        self.diffv = gradient_output * (1.0 - np.power(self.output, 2))\n","        return self.diffv\n"]},{"cell_type":"code","execution_count":240,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:41:53.065750Z","iopub.status.busy":"2024-08-12T17:41:53.065409Z","iopub.status.idle":"2024-08-12T17:41:53.077931Z","shell.execute_reply":"2024-08-12T17:41:53.076848Z","shell.execute_reply.started":"2024-08-12T17:41:53.065722Z"},"trusted":true},"outputs":[],"source":["class Softmax:\n","    def forward(self, inputs):\n","        \"\"\"\n","        Perform the forward pass of the Softmax activation function.\n","\n","        Args:\n","            inputs (np.ndarray): Input data of shape (batch_size, num_classes).\n","\n","        Returns:\n","            np.ndarray: Output data after applying the Softmax activation function, of the same shape as the input.\n","        \"\"\"\n","        # Compute exponentiated values, with numerical stability\n","        exp_inputs = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n","        # normalize by the sum of all the exp values\n","        self.outputs = exp_inputs / np.sum(exp_inputs, axis=1, keepdims=True)\n","        return self.outputs\n","\n","    def backward(self, d_outputs):\n","        \"\"\"\n","        Perform the backward pass of the Softmax activation function to compute gradients.\n","\n","        Args:\n","            d_outputs (np.ndarray): Gradient of the loss with respect to the output of this layer, of shape (batch_size, num_classes).\n","\n","        Returns:\n","            np.ndarray: Gradient of the loss with respect to the input of this layer, of the same shape as the input.\n","        \"\"\"\n","        batch_size = self.outputs.shape[0]\n","        num_classes = self.outputs.shape[1]\n","\n","        # Initialize gradient of the input\n","        d_inputs = np.zeros_like(d_outputs)\n","\n","        # Compute gradients for each sample in the batch\n","        for i in range(batch_size):\n","            single_output = self.outputs[i].reshape(-1, 1)\n","            single_grad_output = d_outputs[i]\n","\n","            # Jacobian matrix for the softmax function\n","            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n","            # Compute gradient for the current sample\n","            d_inputs[i] = np.dot(jacobian_matrix, single_grad_output)\n","\n","        return d_inputs\n"]},{"cell_type":"code","execution_count":241,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:41:53.079765Z","iopub.status.busy":"2024-08-12T17:41:53.079381Z","iopub.status.idle":"2024-08-12T17:41:53.093833Z","shell.execute_reply":"2024-08-12T17:41:53.092815Z","shell.execute_reply.started":"2024-08-12T17:41:53.079736Z"},"trusted":true},"outputs":[],"source":["class CrossEntropyLoss:\n","    def forward(self, targets, predictions):\n","        \"\"\"\n","        Perform the forward pass of the Cross-Entropy Loss function.\n","\n","        Args:\n","            targets (np.ndarray): True labels, one-hot encoded, of shape (batch_size, num_classes).\n","            predictions (np.ndarray): Predicted probabilities, of shape (batch_size, num_classes).\n","\n","        Returns:\n","            float: The computed cross-entropy loss.\n","        \"\"\"\n","        # Ensure numerical stability by subtracting the max value from predictions\n","        p_max = np.max(predictions, axis=1, keepdims=True)\n","        exps = np.exp(predictions - p_max)\n","        self.softmax = exps / np.sum(exps, axis=1, keepdims=True)\n","        self.targets = targets\n","\n","        # Compute the loss using the cross-entropy formula\n","        batch_size = predictions.shape[0]\n","        self.loss = -np.sum(targets * np.log(self.softmax + 1e-10)) / batch_size  # Add epsilon for numerical stability\n","        return self.loss\n","\n","    def backward(self):\n","        \"\"\"\n","        Perform the backward pass of the Cross-Entropy Loss function to compute gradients.\n","\n","        Returns:\n","            np.ndarray: Gradient of the loss with respect to the predictions, of shape (batch_size, num_classes).\n","        \"\"\"\n","        batch_size = self.targets.shape[0]\n","        # Compute gradient of the loss with respect to the predictions\n","        d_predictions = (self.softmax - self.targets) / batch_size\n","        return d_predictions\n","\n"]},{"cell_type":"code","execution_count":252,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:43:34.648708Z","iopub.status.busy":"2024-08-12T17:43:34.647664Z","iopub.status.idle":"2024-08-12T17:43:34.659756Z","shell.execute_reply":"2024-08-12T17:43:34.658458Z","shell.execute_reply.started":"2024-08-12T17:43:34.648669Z"},"trusted":true},"outputs":[],"source":["class SGD:\n","    def __init__(self, learning_rate):\n","        \"\"\"\n","        Initialize the Stochastic Gradient Descent (SGD) optimizer.\n","\n","        Args:\n","            learning_rate (float): Learning rate for the optimizer.\n","        \"\"\"\n","        self.learning_rate = learning_rate\n","\n","    def step(self, layers):\n","        \"\"\"\n","        Perform a single optimization step by updating the weights and biases of the given layers.\n","\n","        Args:\n","            layers (list of objects): List of layers in the network. Each layer should have attributes\n","                                      `W` (weights), `b` (biases), `dW` (gradient of weights), and `db` (gradient of biases).\n","\n","        This method iterates over each layer and updates its weights and biases using the computed gradients.\n","        \"\"\"\n","        for layer in layers:\n","            if hasattr(layer, 'W'):\n","                # Update weights and biases for layers with weight and bias attributes\n","                layer.W -= self.learning_rate * layer.dW\n","                layer.b -= self.learning_rate * layer.db\n","\n","class one_hot:\n","    def __init__(self,num_classes):\n","        self.num_classes=num_classes\n","        \n","    \n","    def one_hot_to_label(self,one_hot_matrix):\n","        \"\"\"\n","        Convert a one-hot encoded matrix to class labels.\n","\n","        Args:\n","            y_one_hot (np.ndarray): One-hot encoded array of shape (num_samples, num_classes).\n","\n","        Returns:\n","            np.ndarray: Array of class labels of shape (num_samples,).\n","        \"\"\"\n","        return np.argmax(one_hot_matrix, axis=1)\n","\n","    def convert_to_one_hot(self,vector):\n","        \"\"\"\n","        Convert a vector of integer class labels to one-hot encoded format.\n","\n","        Args:\n","            vector (np.ndarray): 1-D array of integer class labels, shape (num_samples,).\n","            num_classes (int, optional): Number of classes. If None, it is set to the maximum value in the vector + 1.\n","\n","        Returns:\n","            np.ndarray: 2-D array of one-hot encoded labels, shape (num_samples, num_classes).\n","        \"\"\"\n","        result = np.zeros((len(vector), self.num_classes), dtype=int)\n","        result[np.arange(len(vector)), vector] = 1\n","        return result\n","  "]},{"cell_type":"code","execution_count":251,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:43:33.109642Z","iopub.status.busy":"2024-08-12T17:43:33.109177Z","iopub.status.idle":"2024-08-12T17:43:33.131923Z","shell.execute_reply":"2024-08-12T17:43:33.130771Z","shell.execute_reply.started":"2024-08-12T17:43:33.109605Z"},"trusted":true},"outputs":[],"source":["class Model:\n","    def __init__(self):\n","        \"\"\"\n","        Initialize the Model class.\n","        This class manages the layers, loss function, and optimizer for training and inference.\n","        \"\"\"\n","        self.layers = []\n","\n","    def add_layer(self, layer):\n","        \"\"\"\n","        Add a layer to the model.\n","\n","        Args:\n","            layer (object): A layer object that has `forward` and `backward` methods. The layer should \n","                            also have attributes like `W` and `b` if it contains learnable parameters.\n","        \"\"\"\n","        self.layers.append(layer)\n","\n","    def compile(self, loss, optimizer):\n","        \"\"\"\n","        Compile the model by specifying the loss function and optimizer.\n","\n","        Args:\n","            loss (object): An instance of a loss class that has `forward` and `backward` methods.\n","            optimizer (object): An instance of an optimizer class that has a `step` method.\n","        \"\"\"\n","        self.loss = loss\n","        self.optimizer = optimizer\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Perform a forward pass through the model.\n","\n","        Args:\n","            X (np.ndarray): Input data of shape (batch_size, ...).\n","\n","        Returns:\n","            np.ndarray: The output of the model after passing through all layers.\n","        \"\"\"\n","        for layer in self.layers:\n","            X = layer.forward(X)\n","        return X\n","\n","    def backward(self, dA):\n","        \"\"\"\n","        Perform a backward pass through the model to compute gradients.\n","\n","        Args:\n","            dA (np.ndarray): Gradient of the loss with respect to the model's output.\n","\n","        Returns:\n","            None: The method updates the gradients of the layers in place.\n","        \"\"\"\n","        for layer in reversed(self.layers):\n","            dA = layer.backward(dA)\n","\n","    def train(self, X, y, epochs, batch_size):\n","        \"\"\"\n","        Train the model using mini-batch gradient descent.\n","\n","        Args:\n","            X (np.ndarray): Training data of shape (num_samples, ...).\n","            y (np.ndarray): True labels, one-hot encoded, of shape (num_samples, num_classes).\n","            epochs (int): Number of training epochs.\n","            batch_size (int): Size of each mini-batch.\n","        \"\"\"\n","        num_samples = X.shape[0]\n","        for epoch in range(epochs):\n","            for i in range(0, num_samples, batch_size):\n","                X_batch = X[i:i+batch_size]\n","                y_batch = y[i:i+batch_size]\n","\n","                # Forward pass\n","                y_pred = self.forward(X_batch)\n","\n","                # Compute loss\n","                loss = self.loss.forward(y_batch, y_pred)\n","\n","                # Backward pass\n","                dA = self.loss.backward()\n","                self.backward(dA)\n","\n","                # Update weights and biases\n","                self.optimizer.step(self.layers)\n","\n","            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Make predictions using the trained model.\n","\n","        Args:\n","            X (np.ndarray): Input data of shape (num_samples, ...).\n","\n","        Returns:\n","            np.ndarray: Predicted probabilities of shape (num_samples, num_classes).\n","        \"\"\"\n","        return self.forward(X)\n","\n","    def evaluate(self, X, y):\n","        \"\"\"\n","        Evaluate the model on a test set.\n","\n","        Args:\n","            X (np.ndarray): Test data of shape (num_samples, ...).\n","            y (np.ndarray): True labels, one-hot encoded, of shape (num_samples, num_classes).\n","\n","        Returns:\n","            tuple: A tuple containing:\n","                - np.ndarray: Predicted probabilities of shape (num_samples, num_classes).\n","                - float: Loss value on the test set.\n","                - float: Accuracy percentage on the test set.\n","        \"\"\"\n","        y_pred = self.predict(X)\n","        loss = self.loss.forward(y, y_pred)\n","        accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1)) * 100\n","        return loss, accuracy\n","\n","    def save(self, path):\n","        \"\"\"\n","        Save the model weights to a file.\n","\n","        Args:\n","            path (str): Path to the file where the model weights and bias will be saved.\n","        \"\"\"\n","        weights_bias = {}\n","        for i, layer in enumerate(self.layers):\n","            if hasattr(layer, 'W'):\n","                weights_bias[f'W{i+1}'] = layer.W\n","                weights_bias[f'b{i+1}'] = layer.b\n","        np.savez(path, **weights_bias)\n","\n","    def load(self, path):\n","        \"\"\"\n","        Load a model from a file.\n","\n","        Args:\n","            path (str): Path to the file where the model is saved.\n","\n","        Returns:\n","            Model: The loaded model\n","        \"\"\"\n","        data = np.load(path)\n","        weights_bias = {key: data[key] for key in data.files}\n","        \n","        for i, layer in enumerate(self.layers):\n","            if hasattr(layer, 'W'):\n","                key_W = f'W{i+1}'\n","                key_b = f'b{i+1}'\n","                layer.W = weights_bias[key_W]\n","                layer.b = weights_bias[key_b]"]},{"cell_type":"code","execution_count":253,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:43:37.537949Z","iopub.status.busy":"2024-08-12T17:43:37.537538Z","iopub.status.idle":"2024-08-12T17:45:04.676220Z","shell.execute_reply":"2024-08-12T17:45:04.674649Z","shell.execute_reply.started":"2024-08-12T17:43:37.537917Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20, Loss: 1.5449\n","Epoch 2/20, Loss: 1.5137\n","Epoch 3/20, Loss: 1.5102\n","Epoch 4/20, Loss: 1.5088\n","Epoch 5/20, Loss: 1.5077\n","Epoch 6/20, Loss: 1.5047\n","Epoch 7/20, Loss: 1.4998\n","Epoch 8/20, Loss: 1.4946\n","Epoch 9/20, Loss: 1.4926\n","Epoch 10/20, Loss: 1.4909\n","Epoch 11/20, Loss: 1.4884\n","Epoch 12/20, Loss: 1.4865\n","Epoch 13/20, Loss: 1.4857\n","Epoch 14/20, Loss: 1.4851\n","Epoch 15/20, Loss: 1.4840\n","Epoch 16/20, Loss: 1.4822\n","Epoch 17/20, Loss: 1.4812\n","Epoch 18/20, Loss: 1.4810\n","Epoch 19/20, Loss: 1.4810\n","Epoch 20/20, Loss: 1.4807\n","Test Loss: 1.5018744449324075, Test Accuracy: 96.16666666666667\n","Epoch 1/20, Loss: 1.4811\n","Epoch 2/20, Loss: 1.4688\n","Epoch 3/20, Loss: 1.4616\n","Epoch 4/20, Loss: 1.4615\n","Epoch 5/20, Loss: 1.4619\n","Epoch 6/20, Loss: 1.4623\n","Epoch 7/20, Loss: 1.4620\n","Epoch 8/20, Loss: 1.4619\n","Epoch 9/20, Loss: 1.4618\n","Epoch 10/20, Loss: 1.4619\n","Epoch 11/20, Loss: 1.4616\n","Epoch 12/20, Loss: 1.4616\n","Epoch 13/20, Loss: 1.4615\n","Epoch 14/20, Loss: 1.4614\n","Epoch 15/20, Loss: 1.4614\n","Epoch 16/20, Loss: 1.4614\n","Epoch 17/20, Loss: 1.4614\n","Epoch 18/20, Loss: 1.4614\n","Epoch 19/20, Loss: 1.4613\n","Epoch 20/20, Loss: 1.4613\n","Test Loss: 1.4756042990686693, Test Accuracy: 98.59523809523809\n"]}],"source":["model = Model()\n","model.add_layer(Linear(784, 128))\n","model.add_layer(ReLU())\n","model.add_layer(Linear(128, 10))\n","model.add_layer(Softmax())\n","\n","\n","loss = CrossEntropyLoss()\n","optimizer = SGD(learning_rate=.1)\n","model.compile(loss, optimizer)\n","one_hot=one_hot(10)\n","\n","y_train_one_hot=one_hot.convert_to_one_hot(y_train)\n","y_test_one_hot=one_hot.convert_to_one_hot(y_test)\n","# Assume x_train, y_train, x_test, y_test are preprocessed and available\n","model.train(x_train, y_train_one_hot, epochs=20, batch_size=64)\n","test_loss, test_accuracy = model.evaluate(x_test, y_test_one_hot)\n","print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n","model.train(x_test,y_test_one_hot,epochs=20,batch_size=42)\n","test_loss, test_accuracy  = model.evaluate(x_test, y_test_one_hot)\n","print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n","test_array=model.predict(test)\n","y_ans=one_hot.one_hot_to_label(test_array)\n","ans=pd.DataFrame({\n","    'ImageId':submission[\"ImageId\"].to_numpy(),\n","    'Label':y_ans\n","})"]},{"cell_type":"code","execution_count":254,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:45:12.998623Z","iopub.status.busy":"2024-08-12T17:45:12.998190Z","iopub.status.idle":"2024-08-12T17:45:13.011943Z","shell.execute_reply":"2024-08-12T17:45:13.010639Z","shell.execute_reply.started":"2024-08-12T17:45:12.998591Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImageId</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>27995</th>\n","      <td>27996</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>27996</th>\n","      <td>27997</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>27997</th>\n","      <td>27998</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>27998</th>\n","      <td>27999</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>27999</th>\n","      <td>28000</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28000 rows × 2 columns</p>\n","</div>"],"text/plain":["       ImageId  Label\n","0            1      2\n","1            2      0\n","2            3      9\n","3            4      9\n","4            5      3\n","...        ...    ...\n","27995    27996      9\n","27996    27997      7\n","27997    27998      3\n","27998    27999      9\n","27999    28000      2\n","\n","[28000 rows x 2 columns]"]},"execution_count":254,"metadata":{},"output_type":"execute_result"}],"source":["ans\n","\n","\n"]},{"cell_type":"code","execution_count":255,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:45:15.300957Z","iopub.status.busy":"2024-08-12T17:45:15.300541Z","iopub.status.idle":"2024-08-12T17:45:15.344654Z","shell.execute_reply":"2024-08-12T17:45:15.343654Z","shell.execute_reply.started":"2024-08-12T17:45:15.300926Z"},"trusted":true},"outputs":[],"source":["ans.to_csv('/kaggle/working/submission.csv',index=False)"]},{"cell_type":"code","execution_count":256,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:45:17.275991Z","iopub.status.busy":"2024-08-12T17:45:17.275566Z","iopub.status.idle":"2024-08-12T17:45:17.285332Z","shell.execute_reply":"2024-08-12T17:45:17.284026Z","shell.execute_reply.started":"2024-08-12T17:45:17.275957Z"},"trusted":true},"outputs":[],"source":["path=\"/kaggle/working/model.npz\"\n","model.save(path)"]},{"cell_type":"code","execution_count":257,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:45:19.019027Z","iopub.status.busy":"2024-08-12T17:45:19.018595Z","iopub.status.idle":"2024-08-12T17:45:19.031635Z","shell.execute_reply":"2024-08-12T17:45:19.030293Z","shell.execute_reply.started":"2024-08-12T17:45:19.018986Z"},"trusted":true},"outputs":[],"source":["model1=Model()\n","model1.add_layer(Linear(784, 128))\n","model1.add_layer(ReLU())\n","model1.add_layer(Linear(128, 10))\n","model1.add_layer(Softmax())\n","model1.load(\"/kaggle/working/model.npz\")"]},{"cell_type":"code","execution_count":258,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T17:45:20.544729Z","iopub.status.busy":"2024-08-12T17:45:20.544314Z","iopub.status.idle":"2024-08-12T17:45:20.617936Z","shell.execute_reply":"2024-08-12T17:45:20.616486Z","shell.execute_reply.started":"2024-08-12T17:45:20.544700Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 1.4756042990686693, Test Accuracy: 98.59523809523809\n"]}],"source":["loss = CrossEntropyLoss()\n","optimizer = SGD(learning_rate=.01)\n","model1.compile(loss, optimizer)\n","test_loss, test_accuracy  = model1.evaluate(x_test, y_test_one_hot)\n","print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":861823,"sourceId":3004,"sourceType":"competition"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
